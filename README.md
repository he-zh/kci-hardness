# kci-hardness
Official code for the paper “[On the Hardness of Conditional Independence Testing In Practice](https://arxiv.org/abs/2512.14000)” (NeurIPS 2025 spotlight).

This repository provides reproducible code for our experiments on the practical performance and limitations of kernel-based conditional independence (KCI) tests under various synthetic data settings.


## Requirements

- Python 3.7+
- NumPy
- Torch
- SciPy

## Experiments
### 1D Synthetic Data 
This experiment evaluates Type I and Type II errors on one-dimensional synthetic data, under the standard setting and power maximization setting of the KCI test.
- Standard KCI: ```python main.py config/1d_standard_kci.yaml```
- Power Maximization: ```python main.py config/1d_power_kci.yaml```
### 3D Synthetic Data:
These configurations test on 3D synthetic data where variables share one coordinate or use separate coordinates. Power maximization method is applied by default.
- Separate coordinate: ```python main.py config/3d_separate_coordinate.yaml```
- Shared coordinate: ```python main.py config/3d_shard_coordinate.yaml```

### Configuration Overview
All experiment parameters are defined in YAML files under config/.
Example fields include:
```
data:
    dim: &dim 1 # dimensions
    beta: 3.0 # dependence frequency for non-CI data
model:
    model_c: 'rbf' # kernel type for kernel C
    model_ca: 'rbf' # kernel type for kernel C->A
    ridge_lambda: 0.01 # regularization parameter used in kernel ridge regression
    is_trainable_c: True # whether to use power maximization
    is_trainable_ca: True # whether to optimize kernel and ridge lambda with leave-one-out cross validation in regression
    early_stopping: False # whether to early stop training of the regrossor
    pval_approx_type: 'wild' # use wild bootstrap for null approximation
    n_wild_bootstrap_samples: 1000 # number of surrogate samples generated by wild bootstrap
experiment:
    n_runs: 100 # number of runs
output:
    output_dir: 'results/' # where to save the p value results
    save_results: True # whether to save the results
```
## Reference & Acknowledgements
Part of the code is adopted and modified from
➡️ [romanpogodin/kernel-ci-testing](https://github.com/romanpogodin/kernel-ci-testing/tree/main)

Please refer to that repository for the original implementation (without power maximization) of kernel-based CI testing methods (KCI, KCI-split, CIRCE) and other baselines (GCM, RBPT).

## Citation
If you find this repository helpful, please consider cite
```
@inproceedings{hehardness2025,
  title={On the Hardness of Conditional Independence Testing In Practice},
  author={He, Zheng and Pogodin, Roman and Li, Yazhe and Deka, Namrata and Gretton, Arthur and Sutherland, Danica J},
  booktitle={The Thirty-ninth Annual Conference on Neural Information Processing Systems},
  year={2025}
}
```
## Contact
For any questions, feel free to contact zhhe@cs.ubc.ca. 
Thanks!
